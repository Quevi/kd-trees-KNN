\documentclass[11 pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

\usepackage{amsmath}
\usepackage{empheq}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[linesnumbered,boxed]{algorithm2e}

\usepackage[left=2cm,right=2cm,top=1.5cm,bottom=1.5cm]{geometry}

\title{Algorithmic Complexity project:\\ Classification of $n$ points in $d$-sized Euclidean space}
\author{Luc Blassel, Romain Gautron}

\begin{document}
\maketitle

\section{Introduction}
\subsection{What is classification?}
\paragraph{}The problem we are trying to resolve is the following:
\begin{itemize}
  \item We have $n$ points in a $d$-dimensional Euclidean space
  \item each of these points is one of two colors: Blue or Red
  \item Some of the points have no assigned colors
\end{itemize}
\paragraph{}The goal of this excercise os to accurately guess the color of the points for which we don't know it. This is a very common problem in machine learning.

\subsection{A simple example}
\paragraph{}


\section{What is used today?}
\subsection{A lot of different classification methods}
\paragraph{}There are hundreds of different methods to be able to classify these points into labels.

\subsection{KNN}
\paragraph{}The principle behind the $k$ nearest neighbours is very simple we try to find the k nearest points to the one we are trying to classify according to some distance function.
\subsubsection{Finding the nearest neighbour}
\paragraph{}The na\"ive approach is very simple to implement, we choose a distance function, calculate the distance of the point we want to classify to all the points in the training set and we return the point for which that distance is minimal.
\paragraph{}The distance used in this nearest neighbour search can be any of a number of functions ($x$ being our known point, $q$ the point we want to classify and $d$ the number of dimensions):
\begin{description}
  \item[Euclidean distance: ]$d(x,q)=\sqrt{\sum^d_{i=1}(x_i-q_i)^2}$
  \item[Manhattan distance: ]$d(x,q)=\sum^d_{i=1}|x_i-q_i|$
  \item[Chebychev distance: ]$d(x,q)=\underset{i}{max}(|x_i-q_i|)$
\end{description}
\paragraph{}In a lot of cases the Euclidean distance is used, so in our implementation we will use it as well.
\subsubsection{Transposing to $k$ nearest neighbours}
\paragraph{}To select the k nearest neighbours we can simply run the nearest neighbour algorithm while keeping a priority queue of points sorted according to their distance and keep the $k$ points with the lowest distances. this can also be acheived by keeping the points in a list along with their distance (rather thatn a priority queue) and sort the list according to distances after calculating all distances.
\paragraph{}Once we have the $k$ nearest neighbours to assign a label to our unclasssified point it is a simple majority vote situation where the label that is the most present in the $k$ nearest neighbours is assigned to the point.\\
We must of course take in to account the value of $k$ if $k=1$ then we are in the nearest neighbour problem, if $k$ is big then what happens to size complexity, what happens if there is a tie in our k nearest neighbours?

\subsubsection{How can we lower the number of calculations?}
\paragraph{}In the na\"ive approach we calculate the ditance of the unknown point to all the other points of the dataset. This is of course very costly computation-wise it would be best to eliminate some possibilities


\subsection{Exact KNN versus approximate KNN}
\paragraph{}



\section{What did we implement?}
\paragraph{}Of we course, as we wanted to be as efficient as possible, we did not implement a naive versoin of the k-NN method. We used k-dimensional trees \emph{(ie. k-d trees)} to help us prune the search space and be able to be more efficient with our computing. k-d trees allow us to partition the k-dimensional Euclidean space into subspaces and organize our known points into a data structure similar to binary-search trees.
\subsection{k-d trees}
\paragraph{}To be show how this structure is created we will use a simple 2-dimensional example, it is simpler to represent and visualize but higher dimensional k-d trees work in the exact same way.
\paragraph{}The dataset we have is the following:
\begin{equation*}
  \{(1, 3),(1, 8), (2, 2), (2, 10), (3, 6), (4, 1), (5, 4), (6, 8), (7, 4), (7, 7), (8, 2), (8, 5), (9, 9)\}
\end{equation*}
\paragraph{}To construct the tree, we will alternatively split on the median of the first dimension and then the second at each level. So on level on we will split on the median of dimension 1, on the second level we will split on the median of the second dimension, on the third level we will split on the first dimension again and son on and so forth until all our points are placed in the tree. Since we always split on the median according to the selected dimesion, we can assure that the tree will be as balanced as it can be, which will speed up any subsequent searching operations in that tree.
\paragraph{}So if we implement this method to our dataset we will have:\\
\begin{enumerate}
  \item the median on the first dimension is $(5,4)$, so this point will be our first node of the tree. On the left subtree we will have all the points for which $x_1<5$ and on the right subtree all the points for which $x_2>=5$.
  \item We now place ourselves in the left subtree of $(5,4)$. We will now split the sub-space according to the secon dimension. The median according to the second dimension is now $(3,6)$ so in the left subtree of $(3,6)$ will be all the points of the sub-space for which $x_2<6$ and on the right subtree all the points for which $x_2>=6$.
  \item We build the tree in a recursive manner building the left subtree first and then the right subtree.
\end{enumerate}
\paragraph{}In the end we end up with the following k-d tree:\\
\begin{center}
  \medskip
\begin{tikzpicture}[draw,circle,fill=none]
  \node[draw,fill=red] (0) at (0,0) {$(5,4)$};

  \node[draw,fill=green] (1) at (-4,-2) {$(3,6)$};
  \node[draw,fill=green] (2) at (4,-2 ) {$(7,7)$};

  \node[draw,fill=red] (3) at (-6,-4) {$(2,2)$};
  \node[draw,fill=red] (4) at (-2,-4) {$(2,10)$};
  \node[draw,fill=red] (5) at (2,-4 ) {$(8,2)$};
  \node[draw,fill=red] (6) at (6,-4 ) {$(9,9)$};

  \node[draw,fill=green] (7) at (-7,-6) {$(1,3)$};
  \node[draw,fill=green] (8) at (-5,-6) {$(4,1)$};
  \node[draw,fill=green] (9) at (-3,-6) {$(1,8)$};
  \node[draw,fill=green] (10) at (1,-6) {$(7,4)$};
  \node[draw,fill=green] (11) at (3,-6) {$(8,2)$};
  \node[draw,fill=green] (12) at (5,-6) {$(6,8)$};

  \draw (0)--(1);
  \draw (0)--(2);
  \draw (1)--(3);
  \draw (1)--(4);
  \draw (2)--(5);
  \draw (2)--(6);
  \draw (3)--(7);
  \draw (3)--(8);
  \draw (4)--(9);
  \draw (5)--(10);
  \draw (5)--(11);
  \draw (6)--(12);
\end{tikzpicture}
\paragraph{}The red nodes are splitted on the first dimension, and the green nodes on the second.
\end{center}
\paragraph{}It is also easy to see how this can be generalized to higher dimensions, the process is identical except that instead of looping on $2$ dimensions, we loop on $d$ dimensions. For instance if we have $3$ dimensions, the first $3$ levels are splitted on their corresponding dimensions and then the subsequent levels of the tree are splitted according to the remainder of the euclidean division $\frac{level}{d}$, so for example the $4^{th}$ level of the tree will be split along the $1^{st}$ dimension.
\paragraph{}So in order to build the k-d tree given a set of points we can follow this algorithm:%\\\medskip\\
\begin{center}
\begin{algorithm}[H]
  \SetKwFunction{FRecurs}{createTree}%

  \FRecurs{\emph{list} points, \emph{int} dimensions, \emph{int} depth, \emph{node} parent}:{
  \BlankLine
  \KwData{For the first iteration depth = 0 and parent = none}
  \BlankLine
  \If{point list is empty}{
    \KwRet\;
    }
  \BlankLine
  \BlankLine
  axis $\leftarrow$ depth\%dimensions\;
  sort(points according to axis)\;
  median $\leftarrow \frac{length(points)}{2}$\;
  \BlankLine
  \BlankLine
  root $\leftarrow$ new node(value = points[median], parent=parent,axis=axis,vsited=False)\;
  root.left $\leftarrow$ \FRecurs{points[:median],dimensions,depth+1,root}\;
  root.right $\leftarrow$ \FRecurs{point[median:],dimesions,depth+1,root}\;
  \BlankLine
  \BlankLine
  \KwRet{root}\;
}
\end{algorithm}
\end{center}
\paragraph{}The node object having a value element (the coordinates of the point), a left subtree, a right subtree and the axis bring the dimension along which it was split. The node object also has a visited boolean attribute that will be useful for search functions later on.
\paragraph{}It is also interesting to note that on line $6$ of the algorithm, the points list is sorted, depending on the sorting method the time complexity will not be the same. We use either shellSort or quicksort in our implementation.

\subsection{k nearest neighbour search}
\paragraph{}Now that we have our data organized as a k-d tree, we must use this data-structure to our avantage as we search for the k nearest neighbours.




\subsection{cross-validation}
\subsection{best case complexity}
\subsection{worst case complexity}
\subsection{results}




\section{Future works to be done}
\subsection{Change sort for tree creation}
\paragraph{}Median of medians

\subsection{Use approximate KNN for selecting hyper-parameters}
\paragraph{}LSH in CV to select k






\end{document}
